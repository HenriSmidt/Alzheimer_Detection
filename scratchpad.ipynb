{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l5/4bf7qdjd6k9b38g7jlnrcf580000gn/T/ipykernel_30646/2326577472.py:46: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  rows = self.df.loc[(id, slice_num)]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import lightning.pytorch as pl\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "class MRIDataset(Dataset):\n",
    "    def __init__(self, dataframe, slice_number, transform=None):\n",
    "        self.df = dataframe.set_index([\"ID\", \"slice_number\"])\n",
    "        self.slice_number = slice_number\n",
    "        self.transform = transform\n",
    "        self.resize = transforms.Resize(\n",
    "            (224, 224)\n",
    "        )  # Ensure images are resized to 224x224\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df.index.unique())\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        id, slice_num = self.df.index.unique()[idx]\n",
    "        images = []\n",
    "        for offset in (-1, 0, 1):\n",
    "            slice_path = self.get_random_path(id, slice_num + offset)\n",
    "            image = Image.open(slice_path).convert(\"L\")\n",
    "            image = self.resize(image)  # Resize the image\n",
    "            image = np.array(image)\n",
    "            images.append(image)\n",
    "\n",
    "        # Stack to create a 3-channel image\n",
    "        image_stack = np.stack(images, axis=-1)  # Shape will be (H, W, C)\n",
    "        image_stack = torch.tensor(image_stack).permute(\n",
    "            2, 0, 1\n",
    "        )  # Convert to (C, H, W) tensor\n",
    "\n",
    "        # Normalize the image stack to [0, 1]\n",
    "        image_stack = image_stack.float() / 255.0\n",
    "\n",
    "        if self.transform:\n",
    "            image_stack = self.transform(image_stack)\n",
    "\n",
    "        return image_stack\n",
    "\n",
    "    def get_random_path(self, id, slice_num):\n",
    "        try:\n",
    "            rows = self.df.loc[(id, slice_num)]\n",
    "            if not rows.empty:\n",
    "                # Randomly select between masked and unmasked if available\n",
    "                row = rows.sample(n=1)\n",
    "                return row[\"path\"].values[0]\n",
    "            else:\n",
    "                # If the specific slice_num doesn't exist, default to the original slice\n",
    "                return (\n",
    "                    self.df.loc[(id, self.slice_number)].sample(n=1)[\"path\"].values[0]\n",
    "                )\n",
    "        except KeyError:\n",
    "            # In case the slice is completely unavailable, use the fallback slice\n",
    "            return self.df.loc[(id, self.slice_number)].sample(n=1)[\"path\"].values[0]\n",
    "\n",
    "\n",
    "class MRIImageDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_path, batch_size=32, slice_number=87):\n",
    "        super().__init__()\n",
    "        self.data_path = data_path\n",
    "        self.batch_size = batch_size\n",
    "        self.slice_number = slice_number\n",
    "        self.transform = transforms.Compose(\n",
    "            [transforms.Normalize(mean=[0.5], std=[0.5])]\n",
    "        )\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        data = pd.read_csv(self.data_path)\n",
    "        train_ids, test_ids = train_test_split(\n",
    "            data[\"ID\"].unique(), test_size=0.2, random_state=42\n",
    "        )\n",
    "\n",
    "        train_df = data[data[\"ID\"].isin(train_ids)]\n",
    "        test_df = data[data[\"ID\"].isin(test_ids)]\n",
    "\n",
    "        self.train_dataset = MRIDataset(\n",
    "            train_df, self.slice_number, transform=self.transform\n",
    "        )\n",
    "        self.test_dataset = MRIDataset(\n",
    "            test_df, self.slice_number, transform=self.transform\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "# Usage\n",
    "data_module = MRIImageDataModule(\n",
    "    data_path=\"Data/metadata_for_preprocessed_files.csv\", slice_number=63\n",
    ")\n",
    "data_module.setup()\n",
    "train_loader = data_module.train_dataloader()\n",
    "for batch in train_loader:\n",
    "    # Each batch should have the dimensions [batch_size, channels, height, width]\n",
    "    print(batch.shape)  # Should output torch.Size([batch_size, 3, 224, 224])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['OAS1_0002_MR1', 'OAS1_0003_MR1'], dtype=object),\n",
       " array(['OAS1_0001_MR1'], dtype=object),\n",
       " 3,\n",
       " 3)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = {\n",
    "    \"ID\": [\n",
    "        \"OAS1_0001_MR1\",\n",
    "        \"OAS1_0001_MR1\",\n",
    "        \"OAS1_0001_MR1\",\n",
    "        \"OAS1_0002_MR1\",\n",
    "        \"OAS1_0002_MR1\",\n",
    "        \"OAS1_0003_MR1\",\n",
    "    ],\n",
    "    \"slice_number\": [62, 63, 64, 63, 64, 63],\n",
    "    \"path\": [\n",
    "        \"Data/OASIS_Extracted/OAS1_0001/OAS1_0001_MR1_mpr_n4_anon_111_t88_gfc_slice_62.jpeg\",\n",
    "        \"Data/OASIS_Extracted/OAS1_0001/OAS1_0001_MR1_mpr_n4_anon_111_t88_gfc_slice_63.jpeg\",\n",
    "        \"Data/OASIS_Extracted/OAS1_0001/OAS1_0001_MR1_mpr_n4_anon_111_t88_gfc_slice_64.jpeg\",\n",
    "        \"Data/OASIS_Extracted/OAS1_0002/OAS1_0002_MR1_mpr_n4_anon_111_t88_gfc_slice_63.jpeg\",\n",
    "        \"Data/OASIS_Extracted/OAS1_0002/OAS1_0002_MR1_mpr_n4_anon_111_t88_gfc_slice_64.jpeg\",\n",
    "        \"Data/OASIS_Extracted/OAS1_0003/OAS1_0003_MR1_mpr_n4_anon_111_t88_gfc_slice_63.jpeg\",\n",
    "    ],\n",
    "    \"CDR\": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "    \"is_masked\": [True, False, True, False, True, False],\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "train_ids, test_ids = train_test_split(\n",
    "    df[\"ID\"].unique(), test_size=0.2, random_state=42\n",
    ")\n",
    "train_df = df[df[\"ID\"].isin(train_ids)]\n",
    "test_df = df[df[\"ID\"].isin(test_ids)]\n",
    "\n",
    "train_size = len(train_df)\n",
    "test_size = len(test_df)\n",
    "\n",
    "train_ids, test_ids, train_size, test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>slice_number</th>\n",
       "      <th>path</th>\n",
       "      <th>CDR</th>\n",
       "      <th>is_masked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OAS1_0002_MR1</td>\n",
       "      <td>63</td>\n",
       "      <td>Data/OASIS_Extracted/OAS1_0002/OAS1_0002_MR1_m...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OAS1_0002_MR1</td>\n",
       "      <td>64</td>\n",
       "      <td>Data/OASIS_Extracted/OAS1_0002/OAS1_0002_MR1_m...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>OAS1_0003_MR1</td>\n",
       "      <td>63</td>\n",
       "      <td>Data/OASIS_Extracted/OAS1_0003/OAS1_0003_MR1_m...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              ID  slice_number  \\\n",
       "3  OAS1_0002_MR1            63   \n",
       "4  OAS1_0002_MR1            64   \n",
       "5  OAS1_0003_MR1            63   \n",
       "\n",
       "                                                path  CDR  is_masked  \n",
       "3  Data/OASIS_Extracted/OAS1_0002/OAS1_0002_MR1_m...  0.0      False  \n",
       "4  Data/OASIS_Extracted/OAS1_0002/OAS1_0002_MR1_m...  0.0       True  \n",
       "5  Data/OASIS_Extracted/OAS1_0003/OAS1_0003_MR1_m...  0.0      False  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l5/4bf7qdjd6k9b38g7jlnrcf580000gn/T/ipykernel_31166/2985792169.py:134: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  rows = self.df.loc[(id, slice_num)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            Unnamed: 0 M/F Hand  Age  Educ  SES  MMSE  CDR  \\\n",
      "ID            slice_number                                                   \n",
      "OAS1_0410_MR1 62                   372   F    R   23   NaN  NaN   NaN  0.0   \n",
      "              62                   372   F    R   23   NaN  NaN   NaN  0.0   \n",
      "\n",
      "                            eTIV  nWBV    ASF  Delay  \\\n",
      "ID            slice_number                             \n",
      "OAS1_0410_MR1 62            1507  0.87  1.165    NaN   \n",
      "              62            1507  0.87  1.165    NaN   \n",
      "\n",
      "                                                                         path  \\\n",
      "ID            slice_number                                                      \n",
      "OAS1_0410_MR1 62            Data/OASIS_Extracted/OAS1_0410/OAS1_0410_MR1_m...   \n",
      "              62            Data/OASIS_Extracted/OAS1_0410/OAS1_0410_MR1_m...   \n",
      "\n",
      "                            is_masked  \n",
      "ID            slice_number             \n",
      "OAS1_0410_MR1 62                False  \n",
      "              62                 True  \n",
      "                            Unnamed: 0 M/F Hand  Age  Educ  SES  MMSE  CDR  \\\n",
      "ID            slice_number                                                   \n",
      "OAS1_0410_MR1 62                   372   F    R   23   NaN  NaN   NaN  0.0   \n",
      "\n",
      "                            eTIV  nWBV    ASF  Delay  \\\n",
      "ID            slice_number                             \n",
      "OAS1_0410_MR1 62            1507  0.87  1.165    NaN   \n",
      "\n",
      "                                                                         path  \\\n",
      "ID            slice_number                                                      \n",
      "OAS1_0410_MR1 62            Data/OASIS_Extracted/OAS1_0410/OAS1_0410_MR1_m...   \n",
      "\n",
      "                            is_masked  \n",
      "ID            slice_number             \n",
      "OAS1_0410_MR1 62                 True  \n",
      "ID             slice_number\n",
      "OAS1_0410_MR1  62              Data/OASIS_Extracted/OAS1_0410/OAS1_0410_MR1_m...\n",
      "Name: path, dtype: object\n",
      "Data/OASIS_Extracted/OAS1_0410/OAS1_0410_MR1_mpr_n3_anon_111_t88_masked_gfc_slice_62.jpeg\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mDer Kernel ist beim Ausführen von Code in der aktuellen Zelle oder einer vorherigen Zelle abgestürzt. \n",
      "\u001b[1;31mBitte überprüfen Sie den Code in der/den Zelle(n), um eine mögliche Fehlerursache zu identifizieren. \n",
      "\u001b[1;31mKlicken Sie <a href='https://aka.ms/vscodeJupyterKernelCrash'>hier</a>, um weitere Informationen zu erhalten. \n",
      "\u001b[1;31mWeitere Informationen finden Sie unter Jupyter <a href='command:jupyter.viewOutput'>Protokoll</a>."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import lightning.pytorch as pl\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "# class MRIDataset(Dataset):\n",
    "#     def __init__(self, dataframe, slice_number, transform=None):\n",
    "#         self.df = dataframe.set_index(['ID', 'slice_number'])\n",
    "#         self.slice_number = slice_number\n",
    "#         self.transform = transform\n",
    "#         self.resize = transforms.Resize((224, 224))  # Ensure images are resized to 224x224 # TODO: better to have an assert to save time, since data should be this size already?\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.df.index.unique())\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         id, slice_num = self.df.index.unique()[idx]\n",
    "#         images = []\n",
    "#         for offset in (-1, 0, 1):\n",
    "#             slice_path = self.get_random_path(id, slice_num + offset)\n",
    "#             image = Image.open(slice_path).convert('L')\n",
    "#             image = np.array(image)\n",
    "#             images.append(image)\n",
    "\n",
    "#         # Stack to create a 3-channel image\n",
    "#         image_stack = np.stack(images, axis=-1)  # Shape will be (H, W, C)\n",
    "#         image_stack = torch.tensor(image_stack).permute(2, 0, 1)  # Convert to (C, H, W) tensor\n",
    "\n",
    "#         # Normalize the image stack to [0, 1]\n",
    "#         image_stack = image_stack.float() / 255.0 # TODO: check if images were already normalized and resized to 224 by 224\n",
    "\n",
    "#         if self.transform:\n",
    "#             image_stack = self.transform(image_stack)\n",
    "\n",
    "#         return image_stack\n",
    "\n",
    "#     def get_random_path(self, id, slice_num):\n",
    "#         try:\n",
    "#             rows = self.df.loc[(id, slice_num)]\n",
    "#             if not rows.empty:\n",
    "#                 # Randomly select between masked and unmasked if available\n",
    "#                 row = rows.sample(n=1)\n",
    "#                 return row['path'].values[0]\n",
    "#             else:\n",
    "#                 # If the specific slice_num doesn't exist, default to the original slice\n",
    "#                 return self.df.loc[(id, self.slice_number)].sample(n=1)['path'].values[0]\n",
    "#         except KeyError:\n",
    "#                print(f\"KeyError: The slice number {slice_num} or id {id} does not exist in the Data.\")\n",
    "#         return None\n",
    "\n",
    "# class MRIImageDataModule(pl.LightningDataModule):\n",
    "#     def __init__(self, data_path, batch_size=32, slice_number=87):\n",
    "#         super().__init__()\n",
    "#         self.data_path = data_path\n",
    "#         self.batch_size = batch_size\n",
    "#         self.slice_number = slice_number\n",
    "\n",
    "#     def setup(self, stage=None):\n",
    "#         data = pd.read_csv(self.data_path)\n",
    "#         train_ids, test_ids = train_test_split(data['ID'].unique(), test_size=0.2, random_state=42)\n",
    "\n",
    "#         train_df = data[data['ID'].isin(train_ids)]\n",
    "#         test_df = data[data['ID'].isin(test_ids)]\n",
    "\n",
    "#         self.train_dataset = MRIDataset(train_df, self.slice_number)\n",
    "#         self.test_dataset = MRIDataset(test_df, self.slice_number)\n",
    "\n",
    "#     def train_dataloader(self):\n",
    "#         return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "#     def val_dataloader(self):\n",
    "#         return DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "# Usage\n",
    "# data_module = MRIImageDataModule(data_path='Data/metadata_for_preprocessed_files.csv', slice_number=63)\n",
    "# data_module.setup()\n",
    "# train_loader = data_module.train_dataloader()\n",
    "# for batch in train_loader:\n",
    "#     # Each batch should have the dimensions [batch_size, channels, height, width]\n",
    "#     print(batch.shape)  # Should output torch.Size([batch_size, 3, 224, 224])\n",
    "#     break\n",
    "\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "class MRIDataset(Dataset):\n",
    "    def __init__(self, dataframe, slice_number, transform=None):\n",
    "        self.slice_number = slice_number\n",
    "        self.transform = transform\n",
    "        self.df = dataframe\n",
    "        self.valid_ids = dataframe[dataframe[\"slice_number\"] == slice_number][\n",
    "            \"ID\"\n",
    "        ].unique()\n",
    "        self.df = self.df[self.df[\"ID\"].isin(self.valid_ids)].set_index(\n",
    "            [\"ID\", \"slice_number\"]\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        id = self.valid_ids[idx]\n",
    "        images = []\n",
    "\n",
    "        for offset in (-1, 0, 1):\n",
    "            slice_path = self.get_random_path(id, self.slice_number + offset)\n",
    "            image = Image.open(slice_path).convert(\"L\")\n",
    "            image = np.array(image)\n",
    "            images.append(image)\n",
    "\n",
    "        # Stack to create a 3-channel image\n",
    "        image_stack = np.stack(images, axis=-1)  # Shape will be (H, W, C)\n",
    "        image_stack = torch.tensor(image_stack).permute(\n",
    "            2, 0, 1\n",
    "        )  # Convert to (C, H, W) tensor\n",
    "\n",
    "        # Normalize the image stack to [0, 1]\n",
    "        image_stack = (\n",
    "            image_stack.float() / 255.0\n",
    "        )  # TODO: check if images are already normalized and sized to 224 224\n",
    "\n",
    "        if self.transform:\n",
    "            image_stack = self.transform(image_stack)\n",
    "\n",
    "        return image_stack\n",
    "\n",
    "    def get_random_path(self, id, slice_num):\n",
    "        try:\n",
    "            rows = self.df.loc[(id, slice_num)]\n",
    "            if not rows.empty:\n",
    "                # Randomly select between masked and unmasked if available\n",
    "                row = rows.sample(n=1)\n",
    "                return row[\"path\"].values[0]\n",
    "            else:\n",
    "                # If the specific slice_num doesn't exist, default to the original slice\n",
    "                return (\n",
    "                    self.df.loc[(id, self.slice_number)].sample(n=1)[\"path\"].values[0]\n",
    "                )\n",
    "        except KeyError:\n",
    "            # In case the slice is completely unavailable, use the fallback slice\n",
    "            try:\n",
    "                return (\n",
    "                    self.df.loc[(id, self.slice_number)].sample(n=1)[\"path\"].values[0]\n",
    "                )\n",
    "            except KeyError:  #\n",
    "                print(\n",
    "                    f\"KeyError: The slice number {self.slice_number} or id {id} does not exist in the Data.\"\n",
    "                )\n",
    "        return None\n",
    "\n",
    "\n",
    "class MRIImageDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_path, batch_size=32, slice_number=87):\n",
    "        super().__init__()\n",
    "        self.data_path = data_path\n",
    "        self.batch_size = batch_size\n",
    "        self.slice_number = slice_number\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        data = pd.read_csv(self.data_path)\n",
    "\n",
    "        # Filter to only include IDs with the specified slice_number\n",
    "        data = data[\n",
    "            data[\"slice_number\"].isin(\n",
    "                [self.slice_number - 1, self.slice_number, self.slice_number + 1]\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        train_ids, test_ids = train_test_split(\n",
    "            data[\"ID\"].unique(), test_size=0.2, random_state=42\n",
    "        )\n",
    "\n",
    "        train_df = data[data[\"ID\"].isin(train_ids)]\n",
    "        test_df = data[data[\"ID\"].isin(test_ids)]\n",
    "\n",
    "        self.train_dataset = MRIDataset(train_df, self.slice_number)\n",
    "        self.test_dataset = MRIDataset(test_df, self.slice_number)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "# Usage example\n",
    "# data_module = MRIImageDataModule(data_path='Data/metadata_for_preprocessed_files.csv', slice_number=63)\n",
    "# data_module.setup()\n",
    "# train_loader = data_module.train_dataloader()\n",
    "# for batch in train_loader:\n",
    "#     print(batch.shape)  # Should output torch.Size([batch_size, 3, 224, 224])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/henrismidt/anaconda3/envs/alzheimer/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "941454dceb924d58afaacd088d9fe27c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pixel_values': tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]])}\n"
     ]
    }
   ],
   "source": [
    "from transformers import MobileViTImageProcessor\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# Load the image processor\n",
    "processor = MobileViTImageProcessor.from_pretrained(\"apple/mobilevit-xx-small\")\n",
    "\n",
    "# Load an image\n",
    "image_path = \"Data/Mean_Images/average_slice_0_masked_False.jpeg\"\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "# Preprocess the image\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "print(inputs[\"pixel_values\"].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MobileViTForImageClassification were not initialized from the model checkpoint at apple/mobilevit-xx-small and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([1000, 320]) in the checkpoint and torch.Size([4, 320]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([4]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/henrismidt/anaconda3/envs/alzheimer/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "\n",
      "  | Name  | Type                            | Params\n",
      "----------------------------------------------------------\n",
      "0 | model | MobileViTForImageClassification | 952 K \n",
      "----------------------------------------------------------\n",
      "952 K     Trainable params\n",
      "0         Non-trainable params\n",
      "952 K     Total params\n",
      "3.809     Total estimated model params size (MB)\n",
      "/Users/henrismidt/anaconda3/envs/alzheimer/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63d578cdab0648039c96ff5621dc0fe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/henrismidt/anaconda3/envs/alzheimer/lib/python3.8/site-packages/pytorch_lightning/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "import pytorch_lightning as pl\n",
    "from transformers import MobileViTForImageClassification, MobileViTImageProcessor\n",
    "\n",
    "# Custom dataset class to filter CIFAR-10 classes\n",
    "\n",
    "\n",
    "class CIFAR10Subset(Dataset):\n",
    "    def __init__(\n",
    "        self, root, train=True, transform=None, download=False, class_subset=None\n",
    "    ):\n",
    "        self.dataset = CIFAR10(\n",
    "            root=root, train=train, transform=transform, download=download\n",
    "        )\n",
    "        self.class_subset = class_subset\n",
    "        if class_subset is not None:\n",
    "            self.indices = [\n",
    "                i\n",
    "                for i, label in enumerate(self.dataset.targets)\n",
    "                if label in class_subset\n",
    "            ]\n",
    "        else:\n",
    "            self.indices = list(range(len(self.dataset)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        index = self.indices[idx]\n",
    "        image, label = self.dataset[index]\n",
    "        label = self.class_subset.index(label)  # Reindex the label to [0, 1, 2, 3]\n",
    "        return image, label\n",
    "\n",
    "\n",
    "# Define the LightningModule\n",
    "\n",
    "\n",
    "class MobileViTLightning(pl.LightningModule):\n",
    "    def __init__(self, model_ckpt, num_labels):\n",
    "        super(MobileViTLightning, self).__init__()\n",
    "        self.model = MobileViTForImageClassification.from_pretrained(\n",
    "            model_ckpt, num_labels=num_labels, ignore_mismatched_sizes=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x).logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        outputs = self(inputs)\n",
    "        loss = torch.nn.functional.cross_entropy(outputs, labels)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=2e-5)\n",
    "\n",
    "\n",
    "# Load the pretrained model and processor\n",
    "model_ckpt = \"apple/mobilevit-xx-small\"\n",
    "processor = MobileViTImageProcessor.from_pretrained(model_ckpt)\n",
    "\n",
    "# Load and preprocess the CIFAR-10 dataset\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Use only the first 4 classes for demonstration\n",
    "class_subset = [0, 1, 2, 3]\n",
    "train_dataset = CIFAR10Subset(\n",
    "    root=\"./data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform,\n",
    "    class_subset=class_subset,\n",
    ")\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Initialize the model and trainer\n",
    "model = MobileViTLightning(model_ckpt=model_ckpt, num_labels=4)\n",
    "trainer = pl.Trainer(max_epochs=3)\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(model, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MobileViTForImageClassification were not initialized from the model checkpoint at apple/mobilevit-xx-small and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([1000, 320]) in the checkpoint and torch.Size([4, 320]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([4]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type                            | Params\n",
      "----------------------------------------------------------\n",
      "0 | model | MobileViTForImageClassification | 952 K \n",
      "----------------------------------------------------------\n",
      "952 K     Trainable params\n",
      "0         Non-trainable params\n",
      "952 K     Total params\n",
      "3.809     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a2aaa7708c14e89944d159e2b90c62e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision import transforms\n",
    "import pytorch_lightning as pl\n",
    "from transformers import MobileViTForImageClassification, MobileViTImageProcessor\n",
    "\n",
    "# Custom dataset class to filter CIFAR-10 classes and use MobileViTImageProcessor\n",
    "\n",
    "\n",
    "class CIFAR10Subset(Dataset):\n",
    "    def __init__(\n",
    "        self, root, train=True, transform=None, download=False, class_subset=None\n",
    "    ):\n",
    "        self.dataset = CIFAR10(\n",
    "            root=root, train=train, transform=None, download=download\n",
    "        )\n",
    "        self.transform = transform\n",
    "        self.class_subset = class_subset\n",
    "        if class_subset is not None:\n",
    "            self.indices = [\n",
    "                i\n",
    "                for i, label in enumerate(self.dataset.targets)\n",
    "                if label in class_subset\n",
    "            ]\n",
    "        else:\n",
    "            self.indices = list(range(len(self.dataset)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        index = self.indices[idx]\n",
    "        image, label = self.dataset[index]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        label = self.class_subset.index(label)  # Reindex the label to [0, 1, 2, 3]\n",
    "        return image, label\n",
    "\n",
    "\n",
    "# Define the LightningModule\n",
    "\n",
    "\n",
    "class MobileViTLightning(pl.LightningModule):\n",
    "    def __init__(self, model_ckpt, num_labels):\n",
    "        super(MobileViTLightning, self).__init__()\n",
    "        self.model = MobileViTForImageClassification.from_pretrained(\n",
    "            model_ckpt, num_labels=num_labels, ignore_mismatched_sizes=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x).logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        outputs = self(inputs)\n",
    "        loss = torch.nn.functional.cross_entropy(outputs, labels)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=2e-5)\n",
    "\n",
    "\n",
    "# Load the pretrained model and processor\n",
    "model_ckpt = \"apple/mobilevit-xx-small\"\n",
    "processor = MobileViTImageProcessor.from_pretrained(model_ckpt)\n",
    "\n",
    "# Load and preprocess the CIFAR-10 dataset\n",
    "\n",
    "\n",
    "def transform(image):\n",
    "    # Use MobileViTImageProcessor for preprocessing\n",
    "    return processor(image, return_tensors=\"pt\")[\"pixel_values\"].squeeze(0)\n",
    "\n",
    "\n",
    "# Use only the first 4 classes for demonstration\n",
    "class_subset = [0, 1, 2, 3]\n",
    "train_dataset = CIFAR10Subset(\n",
    "    root=\"./data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform,\n",
    "    class_subset=class_subset,\n",
    ")\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Initialize the model and trainer\n",
    "model = MobileViTLightning(model_ckpt=model_ckpt, num_labels=4)\n",
    "trainer = pl.Trainer(max_epochs=3)\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(model, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs shape: torch.Size([4, 3, 256, 256])\n",
      "Labels: tensor([1, 3, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    inputs, labels = batch\n",
    "    print(\"Inputs shape:\", inputs.shape)\n",
    "    print(\"Labels:\", labels)\n",
    "    break  # Exit after the first batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Class Distribution:\n",
      " CRD\n",
      "Class1    0.342777\n",
      "Class3    0.332754\n",
      "Class2    0.324469\n",
      "Name: proportion, dtype: float64\n",
      "Validation Class Distribution:\n",
      " CRD\n",
      "Class2    0.350919\n",
      "Class3    0.331735\n",
      "Class1    0.317346\n",
      "Name: proportion, dtype: float64\n",
      "Test Class Distribution:\n",
      " CRD\n",
      "Class2    0.349921\n",
      "Class1    0.327804\n",
      "Class3    0.322275\n",
      "Name: proportion, dtype: float64\n",
      "ID Uniqueness Across Datasets: True\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create a mock dataset for testing\n",
    "np.random.seed(42)\n",
    "data = pd.DataFrame(\n",
    "    {\n",
    "        \"ID\": np.random.choice([f\"ID{n}\" for n in range(200)], size=10000),\n",
    "        \"CDR\": np.random.choice([\"Class1\", \"Class2\", \"Class3\"], size=10000),\n",
    "        \"Feature\": np.random.rand(10000),\n",
    "    }\n",
    ")\n",
    "\n",
    "# Define the function for stratified group split\n",
    "\n",
    "\n",
    "def stratified_group_split(\n",
    "    data, group_col, stratify_col, test_size=0.125, random_state=42\n",
    "):\n",
    "    unique_ids = data[group_col].unique()\n",
    "    stratify_values = data.groupby(group_col)[stratify_col].first().values\n",
    "    train_val_ids, test_ids = train_test_split(\n",
    "        unique_ids,\n",
    "        test_size=test_size,\n",
    "        stratify=stratify_values,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "    return train_val_ids, test_ids\n",
    "\n",
    "\n",
    "# Perform initial stratified split\n",
    "train_val_ids, test_ids = stratified_group_split(data, \"ID\", \"CDR\", test_size=0.125)\n",
    "\n",
    "# Further stratified split train + validation\n",
    "train_val_df = data[data[\"ID\"].isin(train_val_ids)]\n",
    "unique_train_val_ids = train_val_df[\"ID\"].unique()\n",
    "stratify_train_val_values = train_val_df.groupby(\"ID\")[\"CDR\"].first().values\n",
    "\n",
    "train_ids, val_ids = train_test_split(\n",
    "    unique_train_val_ids,\n",
    "    test_size=0.142857,\n",
    "    stratify=stratify_train_val_values,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Create final DataFrames\n",
    "train_df = data[data[\"ID\"].isin(train_ids)]\n",
    "val_df = data[data[\"ID\"].isin(val_ids)]\n",
    "test_df = data[data[\"ID\"].isin(test_ids)]\n",
    "\n",
    "# Function to check class distribution\n",
    "\n",
    "\n",
    "def check_class_distribution(df, label_col):\n",
    "    return df[label_col].value_counts(normalize=True)\n",
    "\n",
    "\n",
    "# Function to check ID uniqueness across datasets\n",
    "\n",
    "\n",
    "def check_id_uniqueness(train_ids, val_ids, test_ids):\n",
    "    return len(set(train_ids) & set(val_ids) & set(test_ids)) == 0\n",
    "\n",
    "\n",
    "# Test class distribution\n",
    "train_class_dist = check_class_distribution(train_df, \"CDR\")\n",
    "val_class_dist = check_class_distribution(val_df, \"CDR\")\n",
    "test_class_dist = check_class_distribution(test_df, \"CDR\")\n",
    "\n",
    "# Test ID uniqueness\n",
    "id_uniqueness = check_id_uniqueness(train_ids, val_ids, test_ids)\n",
    "\n",
    "# Print results\n",
    "print(\"Train Class Distribution:\\n\", train_class_dist)\n",
    "print(\"Validation Class Distribution:\\n\", val_class_dist)\n",
    "print(\"Test Class Distribution:\\n\", test_class_dist)\n",
    "print(\"ID Uniqueness Across Datasets:\", id_uniqueness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Distribution in Training Set:\n",
      "CDR\n",
      "0.0    77.300613\n",
      "0.5    15.030675\n",
      "1.0     7.055215\n",
      "2.0     0.613497\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Class Distribution in Validation Set:\n",
      "CDR\n",
      "0.0    78.181818\n",
      "0.5    18.181818\n",
      "1.0     3.636364\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Class Distribution in Test Set:\n",
      "CDR\n",
      "0.0    74.545455\n",
      "0.5    20.000000\n",
      "1.0     5.454545\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from dataset import stratified_group_split\n",
    "\n",
    "\n",
    "# Load the original data\n",
    "data_path = \"Data/metadata_for_preprocessed_files.csv\"  # Update the path if needed\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Filter to only include IDs with the specified slice_number\n",
    "slice_number = 87\n",
    "data = data[\n",
    "    data[\"slice_number\"].isin([slice_number - 1, slice_number, slice_number + 1])\n",
    "]\n",
    "\n",
    "# Initial stratified split: 87.5% train + validation, 12.5% test\n",
    "train_val_ids, test_ids = stratified_group_split(data, \"ID\", \"CDR\", test_size=0.125)\n",
    "\n",
    "# Creating the train + validation DataFrame for further splitting\n",
    "train_val_df = data[data[\"ID\"].isin(train_val_ids)]\n",
    "\n",
    "# Further stratified split train + validation into 75% train and 12.5% validation (relative to the total dataset)\n",
    "unique_train_val_ids = train_val_df[\"ID\"].unique()\n",
    "stratify_train_val_values = train_val_df.groupby(\"ID\")[\"CDR\"].first().values\n",
    "\n",
    "train_ids, val_ids = train_test_split(\n",
    "    unique_train_val_ids,\n",
    "    test_size=0.142857,\n",
    "    stratify=stratify_train_val_values,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Creating the final DataFrames\n",
    "train_df = data[data[\"ID\"].isin(train_ids)]\n",
    "val_df = data[data[\"ID\"].isin(val_ids)]\n",
    "test_df = data[data[\"ID\"].isin(test_ids)]\n",
    "\n",
    "# Function to display class distribution\n",
    "\n",
    "\n",
    "def display_class_distribution(df, label_col):\n",
    "    class_counts = df[label_col].value_counts(normalize=True) * 100\n",
    "    print(class_counts)\n",
    "\n",
    "\n",
    "# Display class distributions\n",
    "print(\"Class Distribution in Training Set:\")\n",
    "display_class_distribution(train_df, \"CDR\")\n",
    "\n",
    "print(\"\\nClass Distribution in Validation Set:\")\n",
    "display_class_distribution(val_df, \"CDR\")\n",
    "\n",
    "print(\"\\nClass Distribution in Test Set:\")\n",
    "display_class_distribution(test_df, \"CDR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from cycler import cycler\n",
    "\n",
    "# Define a more subtle professional color cycle\n",
    "subtle_colors = [\"#4C566A\", \"#D08770\", \"#A3BE8C\", \"#EBCB8B\", \"#81A1C1\", \"#B48EAD\"]\n",
    "\n",
    "# Set the color cycle\n",
    "plt.rc(\"axes\", prop_cycle=(cycler(\"color\", subtle_colors)))\n",
    "\n",
    "# Displaying the color cycle for reference\n",
    "fig, ax = plt.subplots()\n",
    "for i, color in enumerate(subtle_colors):\n",
    "    ax.plot([0, 1], [i, i], label=f\"Line {i+1}\", color=color, linewidth=4)\n",
    "ax.set_yticks(range(len(subtle_colors)))\n",
    "ax.set_yticklabels([f\"Color {i+1}\" for i in range(len(subtle_colors))])\n",
    "ax.legend()\n",
    "plt.title(\"Subtle Professional Color Cycle\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.65098039, 0.80784314, 0.89019608, 1.        ],\n",
       "       [0.12156863, 0.47058824, 0.70588235, 1.        ],\n",
       "       [0.69803922, 0.8745098 , 0.54117647, 1.        ],\n",
       "       [0.2       , 0.62745098, 0.17254902, 1.        ],\n",
       "       [0.98431373, 0.60392157, 0.6       , 1.        ],\n",
       "       [0.89019608, 0.10196078, 0.10980392, 1.        ],\n",
       "       [0.99215686, 0.74901961, 0.43529412, 1.        ],\n",
       "       [1.        , 0.49803922, 0.        , 1.        ],\n",
       "       [0.79215686, 0.69803922, 0.83921569, 1.        ],\n",
       "       [0.41568627, 0.23921569, 0.60392157, 1.        ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.cm.Paired(range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alzheimer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
