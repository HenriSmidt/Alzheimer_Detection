{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l5/4bf7qdjd6k9b38g7jlnrcf580000gn/T/ipykernel_30646/2326577472.py:46: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  rows = self.df.loc[(id, slice_num)]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import lightning.pytorch as pl\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "class MRIDataset(Dataset):\n",
    "    def __init__(self, dataframe, slice_number, transform=None):\n",
    "        self.df = dataframe.set_index(['ID', 'slice_number'])\n",
    "        self.slice_number = slice_number\n",
    "        self.transform = transform\n",
    "        self.resize = transforms.Resize((224, 224))  # Ensure images are resized to 224x224\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df.index.unique())\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        id, slice_num = self.df.index.unique()[idx]\n",
    "        images = []\n",
    "        for offset in (-1, 0, 1):\n",
    "            slice_path = self.get_random_path(id, slice_num + offset)\n",
    "            image = Image.open(slice_path).convert('L')\n",
    "            image = self.resize(image)  # Resize the image\n",
    "            image = np.array(image)\n",
    "            images.append(image)\n",
    "        \n",
    "        # Stack to create a 3-channel image\n",
    "        image_stack = np.stack(images, axis=-1)  # Shape will be (H, W, C)\n",
    "        image_stack = torch.tensor(image_stack).permute(2, 0, 1)  # Convert to (C, H, W) tensor\n",
    "        \n",
    "        # Normalize the image stack to [0, 1]\n",
    "        image_stack = image_stack.float() / 255.0\n",
    "        \n",
    "        if self.transform:\n",
    "            image_stack = self.transform(image_stack)\n",
    "            \n",
    "        return image_stack\n",
    "\n",
    "    def get_random_path(self, id, slice_num):\n",
    "        try:\n",
    "            rows = self.df.loc[(id, slice_num)]\n",
    "            if not rows.empty:\n",
    "                # Randomly select between masked and unmasked if available\n",
    "                row = rows.sample(n=1)\n",
    "                return row['path'].values[0]\n",
    "            else:\n",
    "                # If the specific slice_num doesn't exist, default to the original slice\n",
    "                return self.df.loc[(id, self.slice_number)].sample(n=1)['path'].values[0]\n",
    "        except KeyError:\n",
    "            # In case the slice is completely unavailable, use the fallback slice\n",
    "            return self.df.loc[(id, self.slice_number)].sample(n=1)['path'].values[0]\n",
    "\n",
    "\n",
    "class MRIImageDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_path, batch_size=32, slice_number=87):\n",
    "        super().__init__()\n",
    "        self.data_path = data_path\n",
    "        self.batch_size = batch_size\n",
    "        self.slice_number = slice_number\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "        ])\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        data = pd.read_csv(self.data_path)\n",
    "        train_ids, test_ids = train_test_split(data['ID'].unique(), test_size=0.2, random_state=42)\n",
    "\n",
    "        train_df = data[data['ID'].isin(train_ids)]\n",
    "        test_df = data[data['ID'].isin(test_ids)]\n",
    "\n",
    "        self.train_dataset = MRIDataset(train_df, self.slice_number, transform=self.transform)\n",
    "        self.test_dataset = MRIDataset(test_df, self.slice_number, transform=self.transform)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "# Usage\n",
    "data_module = MRIImageDataModule(data_path='Data/metadata_for_preprocessed_files.csv', slice_number=63)\n",
    "data_module.setup()\n",
    "train_loader = data_module.train_dataloader()\n",
    "for batch in train_loader:\n",
    "    # Each batch should have the dimensions [batch_size, channels, height, width]\n",
    "    print(batch.shape)  # Should output torch.Size([batch_size, 3, 224, 224])\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['OAS1_0002_MR1', 'OAS1_0003_MR1'], dtype=object),\n",
       " array(['OAS1_0001_MR1'], dtype=object),\n",
       " 3,\n",
       " 3)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = {\n",
    "    'ID': ['OAS1_0001_MR1', 'OAS1_0001_MR1', 'OAS1_0001_MR1', 'OAS1_0002_MR1', 'OAS1_0002_MR1', 'OAS1_0003_MR1'],\n",
    "    'slice_number': [62, 63, 64, 63, 64, 63],\n",
    "    'path': [\n",
    "        'Data/OASIS_Extracted/OAS1_0001/OAS1_0001_MR1_mpr_n4_anon_111_t88_gfc_slice_62.jpeg',\n",
    "        'Data/OASIS_Extracted/OAS1_0001/OAS1_0001_MR1_mpr_n4_anon_111_t88_gfc_slice_63.jpeg',\n",
    "        'Data/OASIS_Extracted/OAS1_0001/OAS1_0001_MR1_mpr_n4_anon_111_t88_gfc_slice_64.jpeg',\n",
    "        'Data/OASIS_Extracted/OAS1_0002/OAS1_0002_MR1_mpr_n4_anon_111_t88_gfc_slice_63.jpeg',\n",
    "        'Data/OASIS_Extracted/OAS1_0002/OAS1_0002_MR1_mpr_n4_anon_111_t88_gfc_slice_64.jpeg',\n",
    "        'Data/OASIS_Extracted/OAS1_0003/OAS1_0003_MR1_mpr_n4_anon_111_t88_gfc_slice_63.jpeg'\n",
    "    ],\n",
    "    'CDR': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "    'is_masked': [True, False, True, False, True, False]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "train_ids, test_ids = train_test_split(df['ID'].unique(), test_size=0.2, random_state=42)\n",
    "train_df = df[df['ID'].isin(train_ids)]\n",
    "test_df = df[df['ID'].isin(test_ids)]\n",
    "\n",
    "train_size = len(train_df)\n",
    "test_size = len(test_df)\n",
    "\n",
    "train_ids, test_ids, train_size, test_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>slice_number</th>\n",
       "      <th>path</th>\n",
       "      <th>CDR</th>\n",
       "      <th>is_masked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OAS1_0002_MR1</td>\n",
       "      <td>63</td>\n",
       "      <td>Data/OASIS_Extracted/OAS1_0002/OAS1_0002_MR1_m...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OAS1_0002_MR1</td>\n",
       "      <td>64</td>\n",
       "      <td>Data/OASIS_Extracted/OAS1_0002/OAS1_0002_MR1_m...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>OAS1_0003_MR1</td>\n",
       "      <td>63</td>\n",
       "      <td>Data/OASIS_Extracted/OAS1_0003/OAS1_0003_MR1_m...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              ID  slice_number  \\\n",
       "3  OAS1_0002_MR1            63   \n",
       "4  OAS1_0002_MR1            64   \n",
       "5  OAS1_0003_MR1            63   \n",
       "\n",
       "                                                path  CDR  is_masked  \n",
       "3  Data/OASIS_Extracted/OAS1_0002/OAS1_0002_MR1_m...  0.0      False  \n",
       "4  Data/OASIS_Extracted/OAS1_0002/OAS1_0002_MR1_m...  0.0       True  \n",
       "5  Data/OASIS_Extracted/OAS1_0003/OAS1_0003_MR1_m...  0.0      False  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l5/4bf7qdjd6k9b38g7jlnrcf580000gn/T/ipykernel_31166/2985792169.py:134: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  rows = self.df.loc[(id, slice_num)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            Unnamed: 0 M/F Hand  Age  Educ  SES  MMSE  CDR  \\\n",
      "ID            slice_number                                                   \n",
      "OAS1_0410_MR1 62                   372   F    R   23   NaN  NaN   NaN  0.0   \n",
      "              62                   372   F    R   23   NaN  NaN   NaN  0.0   \n",
      "\n",
      "                            eTIV  nWBV    ASF  Delay  \\\n",
      "ID            slice_number                             \n",
      "OAS1_0410_MR1 62            1507  0.87  1.165    NaN   \n",
      "              62            1507  0.87  1.165    NaN   \n",
      "\n",
      "                                                                         path  \\\n",
      "ID            slice_number                                                      \n",
      "OAS1_0410_MR1 62            Data/OASIS_Extracted/OAS1_0410/OAS1_0410_MR1_m...   \n",
      "              62            Data/OASIS_Extracted/OAS1_0410/OAS1_0410_MR1_m...   \n",
      "\n",
      "                            is_masked  \n",
      "ID            slice_number             \n",
      "OAS1_0410_MR1 62                False  \n",
      "              62                 True  \n",
      "                            Unnamed: 0 M/F Hand  Age  Educ  SES  MMSE  CDR  \\\n",
      "ID            slice_number                                                   \n",
      "OAS1_0410_MR1 62                   372   F    R   23   NaN  NaN   NaN  0.0   \n",
      "\n",
      "                            eTIV  nWBV    ASF  Delay  \\\n",
      "ID            slice_number                             \n",
      "OAS1_0410_MR1 62            1507  0.87  1.165    NaN   \n",
      "\n",
      "                                                                         path  \\\n",
      "ID            slice_number                                                      \n",
      "OAS1_0410_MR1 62            Data/OASIS_Extracted/OAS1_0410/OAS1_0410_MR1_m...   \n",
      "\n",
      "                            is_masked  \n",
      "ID            slice_number             \n",
      "OAS1_0410_MR1 62                 True  \n",
      "ID             slice_number\n",
      "OAS1_0410_MR1  62              Data/OASIS_Extracted/OAS1_0410/OAS1_0410_MR1_m...\n",
      "Name: path, dtype: object\n",
      "Data/OASIS_Extracted/OAS1_0410/OAS1_0410_MR1_mpr_n3_anon_111_t88_masked_gfc_slice_62.jpeg\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mDer Kernel ist beim Ausführen von Code in der aktuellen Zelle oder einer vorherigen Zelle abgestürzt. \n",
      "\u001b[1;31mBitte überprüfen Sie den Code in der/den Zelle(n), um eine mögliche Fehlerursache zu identifizieren. \n",
      "\u001b[1;31mKlicken Sie <a href='https://aka.ms/vscodeJupyterKernelCrash'>hier</a>, um weitere Informationen zu erhalten. \n",
      "\u001b[1;31mWeitere Informationen finden Sie unter Jupyter <a href='command:jupyter.viewOutput'>Protokoll</a>."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import lightning.pytorch as pl\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "# class MRIDataset(Dataset):\n",
    "#     def __init__(self, dataframe, slice_number, transform=None):\n",
    "#         self.df = dataframe.set_index(['ID', 'slice_number'])\n",
    "#         self.slice_number = slice_number\n",
    "#         self.transform = transform\n",
    "#         self.resize = transforms.Resize((224, 224))  # Ensure images are resized to 224x224 # TODO: better to have an assert to save time, since data should be this size already?\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.df.index.unique())\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         id, slice_num = self.df.index.unique()[idx]\n",
    "#         images = []\n",
    "#         for offset in (-1, 0, 1):\n",
    "#             slice_path = self.get_random_path(id, slice_num + offset)\n",
    "#             image = Image.open(slice_path).convert('L')\n",
    "#             image = np.array(image)\n",
    "#             images.append(image)\n",
    "        \n",
    "#         # Stack to create a 3-channel image\n",
    "#         image_stack = np.stack(images, axis=-1)  # Shape will be (H, W, C)\n",
    "#         image_stack = torch.tensor(image_stack).permute(2, 0, 1)  # Convert to (C, H, W) tensor\n",
    "        \n",
    "#         # Normalize the image stack to [0, 1]\n",
    "#         image_stack = image_stack.float() / 255.0 # TODO: check if images were already normalized and resized to 224 by 224\n",
    "        \n",
    "#         if self.transform:\n",
    "#             image_stack = self.transform(image_stack)\n",
    "            \n",
    "#         return image_stack\n",
    "\n",
    "#     def get_random_path(self, id, slice_num):\n",
    "#         try:\n",
    "#             rows = self.df.loc[(id, slice_num)]\n",
    "#             if not rows.empty:\n",
    "#                 # Randomly select between masked and unmasked if available\n",
    "#                 row = rows.sample(n=1)\n",
    "#                 return row['path'].values[0]\n",
    "#             else:\n",
    "#                 # If the specific slice_num doesn't exist, default to the original slice\n",
    "#                 return self.df.loc[(id, self.slice_number)].sample(n=1)['path'].values[0]\n",
    "#         except KeyError:\n",
    "#                print(f\"KeyError: The slice number {slice_num} or id {id} does not exist in the Data.\")\n",
    "#         return None\n",
    "\n",
    "# class MRIImageDataModule(pl.LightningDataModule):\n",
    "#     def __init__(self, data_path, batch_size=32, slice_number=87):\n",
    "#         super().__init__()\n",
    "#         self.data_path = data_path\n",
    "#         self.batch_size = batch_size\n",
    "#         self.slice_number = slice_number\n",
    "\n",
    "#     def setup(self, stage=None):\n",
    "#         data = pd.read_csv(self.data_path)\n",
    "#         train_ids, test_ids = train_test_split(data['ID'].unique(), test_size=0.2, random_state=42)\n",
    "\n",
    "#         train_df = data[data['ID'].isin(train_ids)]\n",
    "#         test_df = data[data['ID'].isin(test_ids)]\n",
    "\n",
    "#         self.train_dataset = MRIDataset(train_df, self.slice_number)\n",
    "#         self.test_dataset = MRIDataset(test_df, self.slice_number)\n",
    "\n",
    "#     def train_dataloader(self):\n",
    "#         return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "#     def val_dataloader(self):\n",
    "#         return DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "# Usage\n",
    "# data_module = MRIImageDataModule(data_path='Data/metadata_for_preprocessed_files.csv', slice_number=63)\n",
    "# data_module.setup()\n",
    "# train_loader = data_module.train_dataloader()\n",
    "# for batch in train_loader:\n",
    "#     # Each batch should have the dimensions [batch_size, channels, height, width]\n",
    "#     print(batch.shape)  # Should output torch.Size([batch_size, 3, 224, 224])\n",
    "#     break\n",
    "\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class MRIDataset(Dataset):\n",
    "    def __init__(self, dataframe, slice_number, transform=None):\n",
    "        self.slice_number = slice_number\n",
    "        self.transform = transform\n",
    "        self.df = dataframe\n",
    "        self.valid_ids = dataframe[dataframe['slice_number'] == slice_number]['ID'].unique()\n",
    "        self.df = self.df[self.df['ID'].isin(self.valid_ids)].set_index(['ID', 'slice_number'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        id = self.valid_ids[idx]\n",
    "        images = []\n",
    "        \n",
    "        for offset in (-1, 0, 1):\n",
    "            slice_path = self.get_random_path(id, self.slice_number + offset)\n",
    "            image = Image.open(slice_path).convert('L')\n",
    "            image = np.array(image)\n",
    "            images.append(image)\n",
    "        \n",
    "        # Stack to create a 3-channel image\n",
    "        image_stack = np.stack(images, axis=-1)  # Shape will be (H, W, C)\n",
    "        image_stack = torch.tensor(image_stack).permute(2, 0, 1)  # Convert to (C, H, W) tensor\n",
    "        \n",
    "        # Normalize the image stack to [0, 1]\n",
    "        image_stack = image_stack.float() / 255.0 #TODO: check if images are already normalized and sized to 224 224\n",
    "        \n",
    "        if self.transform:\n",
    "            image_stack = self.transform(image_stack)\n",
    "            \n",
    "        return image_stack\n",
    "    \n",
    "    def get_random_path(self, id, slice_num):\n",
    "        try:\n",
    "            rows = self.df.loc[(id, slice_num)]\n",
    "            if not rows.empty:\n",
    "                # Randomly select between masked and unmasked if available\n",
    "                row = rows.sample(n=1)\n",
    "                return row['path'].values[0]\n",
    "            else:\n",
    "                # If the specific slice_num doesn't exist, default to the original slice\n",
    "                return self.df.loc[(id, self.slice_number)].sample(n=1)['path'].values[0]\n",
    "        except KeyError:\n",
    "            # In case the slice is completely unavailable, use the fallback slice\n",
    "            try: \n",
    "                return self.df.loc[(id, self.slice_number)].sample(n=1)['path'].values[0]\n",
    "            except KeyError: #                \n",
    "                print(f\"KeyError: The slice number {self.slice_number} or id {id} does not exist in the Data.\")\n",
    "        return None\n",
    "\n",
    "class MRIImageDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_path, batch_size=32, slice_number=87):\n",
    "        super().__init__()\n",
    "        self.data_path = data_path\n",
    "        self.batch_size = batch_size\n",
    "        self.slice_number = slice_number\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        data = pd.read_csv(self.data_path)\n",
    "        \n",
    "        # Filter to only include IDs with the specified slice_number\n",
    "        data = data[data['slice_number'].isin([self.slice_number - 1, self.slice_number, self.slice_number + 1])]\n",
    "\n",
    "        train_ids, test_ids = train_test_split(data['ID'].unique(), test_size=0.2, random_state=42)\n",
    "\n",
    "        train_df = data[data['ID'].isin(train_ids)]\n",
    "        test_df = data[data['ID'].isin(test_ids)]\n",
    "\n",
    "        self.train_dataset = MRIDataset(train_df, self.slice_number)\n",
    "        self.test_dataset = MRIDataset(test_df, self.slice_number)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "# Usage example\n",
    "# data_module = MRIImageDataModule(data_path='Data/metadata_for_preprocessed_files.csv', slice_number=63)\n",
    "# data_module.setup()\n",
    "# train_loader = data_module.train_dataloader()\n",
    "# for batch in train_loader:\n",
    "#     print(batch.shape)  # Should output torch.Size([batch_size, 3, 224, 224])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/henrismidt/anaconda3/envs/alzheimer/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "941454dceb924d58afaacd088d9fe27c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pixel_values': tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]])}\n"
     ]
    }
   ],
   "source": [
    "from transformers import MobileViTImageProcessor\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# Load the image processor\n",
    "processor = MobileViTImageProcessor.from_pretrained(\"apple/mobilevit-xx-small\")\n",
    "\n",
    "# Load an image\n",
    "image_path = \"Data/Mean_Images/average_slice_0_masked_False.jpeg\"\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "# Preprocess the image\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "print(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "print(inputs[\"pixel_values\"].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MobileViTForImageClassification were not initialized from the model checkpoint at apple/mobilevit-xx-small and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([1000, 320]) in the checkpoint and torch.Size([4, 320]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([4]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/henrismidt/anaconda3/envs/alzheimer/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "\n",
      "  | Name  | Type                            | Params\n",
      "----------------------------------------------------------\n",
      "0 | model | MobileViTForImageClassification | 952 K \n",
      "----------------------------------------------------------\n",
      "952 K     Trainable params\n",
      "0         Non-trainable params\n",
      "952 K     Total params\n",
      "3.809     Total estimated model params size (MB)\n",
      "/Users/henrismidt/anaconda3/envs/alzheimer/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63d578cdab0648039c96ff5621dc0fe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "import pytorch_lightning as pl\n",
    "from transformers import MobileViTForImageClassification, MobileViTImageProcessor\n",
    "\n",
    "# Custom dataset class to filter CIFAR-10 classes\n",
    "class CIFAR10Subset(Dataset):\n",
    "    def __init__(self, root, train=True, transform=None, download=False, class_subset=None):\n",
    "        self.dataset = CIFAR10(root=root, train=train, transform=transform, download=download)\n",
    "        self.class_subset = class_subset\n",
    "        if class_subset is not None:\n",
    "            self.indices = [i for i, label in enumerate(self.dataset.targets) if label in class_subset]\n",
    "        else:\n",
    "            self.indices = list(range(len(self.dataset)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        index = self.indices[idx]\n",
    "        image, label = self.dataset[index]\n",
    "        label = self.class_subset.index(label)  # Reindex the label to [0, 1, 2, 3]\n",
    "        return image, label\n",
    "\n",
    "# Define the LightningModule\n",
    "class MobileViTLightning(pl.LightningModule):\n",
    "    def __init__(self, model_ckpt, num_labels):\n",
    "        super(MobileViTLightning, self).__init__()\n",
    "        self.model = MobileViTForImageClassification.from_pretrained(model_ckpt, num_labels=num_labels, ignore_mismatched_sizes=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x).logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch\n",
    "        outputs = self(inputs)\n",
    "        loss = torch.nn.functional.cross_entropy(outputs, labels)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=2e-5)\n",
    "\n",
    "# Load the pretrained model and processor\n",
    "model_ckpt = \"apple/mobilevit-xx-small\"\n",
    "processor = MobileViTImageProcessor.from_pretrained(model_ckpt)\n",
    "\n",
    "# Load and preprocess the CIFAR-10 dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Use only the first 4 classes for demonstration\n",
    "class_subset = [0, 1, 2, 3]\n",
    "train_dataset = CIFAR10Subset(root=\"./data\", train=True, download=True, transform=transform, class_subset=class_subset)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Initialize the model and trainer\n",
    "model = MobileViTLightning(model_ckpt=model_ckpt, num_labels=4)\n",
    "trainer = pl.Trainer(max_epochs=3)\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(model, train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alzheimer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
